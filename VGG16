{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG16","provenance":[],"collapsed_sections":["_iU7M31O1C_k"],"authorship_tag":"ABX9TyPuVLqWpFTscReFS/6vq+tU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ERMrlDGgo9Tf"},"source":["#VGG16 - Convolutional Network for Classification and Detection\n","\n","**VGG16** là convolutional neural network model.\n","- The model achieves 92.7% top-5 test accuracy in ImageNet( là một dataset gồm hơn 14 triệu ảnh đươc dán nhãn 1000 classes),\n","- VGG16 được train trong nhiều tuần và dùng nhiề NVIDIA Titan Black GPU để train.\n","\n","**DataSet**\n","- ImageNet là một tập dữ liệu của hơn 15 triệu hình ảnh có độ phân giải cao được dán nhãn thuộc khoảng 22000 categories.\n","- Các hình ảnh được thu thập từ web và được gắn nhãn bởi những người dùng công cụ tìm kiếm Amazon của Mechanical Turk.\n","\n","**The Architecture**\n","\n","![VGG16 Architecture](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg)\n","\n","**Configurations**\n","\n","![ConvNet Configuration](https://neurohive.io/wp-content/uploads/2018/11/Capture-564x570.jpg)\n","\n","\n","\n","**Use-Cases and Implementation**\n","\n","Có 2 nhược điểm lớn của VGGNet:\n","- Khi train tốn rất nhiều thời gian.\n","- Kiến trúc có một lượng khá lớn weights.\n","\n","Bởi vì độ sâu và số lượng FC nodes, VGG16 hơn 533MB. Thường không phù hợp khi áp dụng vào thực tế nhưng phù hợp cho việc học vì nó dễ implement."]},{"cell_type":"markdown","metadata":{"id":"_iU7M31O1C_k"},"source":["##Implementation with tensorflow"]},{"cell_type":"code","metadata":{"id":"3dAgSXkY1BeA"},"source":["import inspect\n","import os\n","\n","import numpy as np\n","import tensorflow as tf\n","import time\n","\n","VGG_MEAN = [103.939, 116.779, 123.68]\n","\n","\n","class Vgg16:\n","    def __init__(self, vgg16_npy_path=None):\n","        if vgg16_npy_path is None:\n","            path = inspect.getfile(Vgg16)\n","            path = os.path.abspath(os.path.join(path, os.pardir))\n","            path = os.path.join(path, \"vgg16.npy\")\n","            vgg16_npy_path = path\n","            print(path)\n","\n","        self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item()\n","        print(\"npy file loaded\")\n","\n","    def build(self, rgb):\n","        \"\"\"\n","        load variable from npy to build the VGG\n","        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n","        \"\"\"\n","\n","        start_time = time.time()\n","        print(\"build model started\")\n","        rgb_scaled = rgb * 255.0\n","\n","        # Convert RGB to BGR\n","        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled)\n","        assert red.get_shape().as_list()[1:] == [224, 224, 1]\n","        assert green.get_shape().as_list()[1:] == [224, 224, 1]\n","        assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n","        bgr = tf.concat(axis=3, values=[\n","            blue - VGG_MEAN[0],\n","            green - VGG_MEAN[1],\n","            red - VGG_MEAN[2],\n","        ])\n","        assert bgr.get_shape().as_list()[1:] == [224, 224, 3]\n","\n","        self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n","        self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n","        self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n","\n","        self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n","        self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n","        self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n","\n","        self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n","        self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n","        self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n","        self.pool3 = self.max_pool(self.conv3_3, 'pool3')\n","\n","        self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n","        self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n","        self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n","        self.pool4 = self.max_pool(self.conv4_3, 'pool4')\n","\n","        self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n","        self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n","        self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n","        self.pool5 = self.max_pool(self.conv5_3, 'pool5')\n","\n","        self.fc6 = self.fc_layer(self.pool5, \"fc6\")\n","        assert self.fc6.get_shape().as_list()[1:] == [4096]\n","        self.relu6 = tf.nn.relu(self.fc6)\n","\n","        self.fc7 = self.fc_layer(self.relu6, \"fc7\")\n","        self.relu7 = tf.nn.relu(self.fc7)\n","\n","        self.fc8 = self.fc_layer(self.relu7, \"fc8\")\n","\n","        self.prob = tf.nn.softmax(self.fc8, name=\"prob\")\n","\n","        self.data_dict = None\n","        print((\"build model finished: %ds\" % (time.time() - start_time)))\n","\n","    def avg_pool(self, bottom, name):\n","        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n","\n","    def max_pool(self, bottom, name):\n","        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n","\n","    def conv_layer(self, bottom, name):\n","        with tf.variable_scope(name):\n","            filt = self.get_conv_filter(name)\n","\n","            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n","\n","            conv_biases = self.get_bias(name)\n","            bias = tf.nn.bias_add(conv, conv_biases)\n","\n","            relu = tf.nn.relu(bias)\n","            return relu\n","\n","    def fc_layer(self, bottom, name):\n","        with tf.variable_scope(name):\n","            shape = bottom.get_shape().as_list()\n","            dim = 1\n","            for d in shape[1:]:\n","                dim *= d\n","            x = tf.reshape(bottom, [-1, dim])\n","\n","            weights = self.get_fc_weight(name)\n","            biases = self.get_bias(name)\n","\n","            # Fully connected layer. Note that the '+' operation automatically\n","            # broadcasts the biases.\n","            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n","\n","            return fc\n","\n","    def get_conv_filter(self, name):\n","        return tf.constant(self.data_dict[name][0], name=\"filter\")\n","\n","    def get_bias(self, name):\n","        return tf.constant(self.data_dict[name][1], name=\"biases\")\n","\n","    def get_fc_weight(self, name):\n","        return tf.constant(self.data_dict[name][0], name=\"weights\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUwbSOqk2Fpw"},"source":["##Implementation with Keras\n"]},{"cell_type":"code","metadata":{"id":"OXuPJ2Db2Osk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621979227342,"user_tz":-420,"elapsed":1374,"user":{"displayName":"Dũng Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gii5U-zs-8ujhQaCpvDo03iequwy5RSKEnhRnwQGQ=s64","userId":"11702303910424597473"}},"outputId":"7a5f9479-71d7-4949-a985-5ec627a106b0"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.utils import np_utils\n","from keras.datasets import mnist\n","\n","model = Sequential()\n","\n","model.add(Conv2D(64, (3, 3),padding='same', activation='relu', input_shape=(224,224,3)))\n","model.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(256, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(256, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(256, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(256, (3, 3),padding='same', activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(521, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(521, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(521, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(521, (3, 3),padding='same', activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Conv2D(521, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(521, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(521, (3, 3),padding='same', activation='relu'))\n","model.add(Conv2D(521, (3, 3),padding='same', activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","model.add(Flatten())\n","\n","model.add(Dense(4096, activation='relu'))\n","model.add(Dense(4096, activation='relu'))\n","model.add(Dense(1000, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='sgd',\n","              metrics=['accuracy'])\n","\n","model.summary()\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_50 (Conv2D)           (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","conv2d_51 (Conv2D)           (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","max_pooling2d_16 (MaxPooling (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","conv2d_52 (Conv2D)           (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","conv2d_53 (Conv2D)           (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","max_pooling2d_17 (MaxPooling (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","conv2d_54 (Conv2D)           (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","conv2d_55 (Conv2D)           (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","conv2d_56 (Conv2D)           (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","conv2d_57 (Conv2D)           (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","max_pooling2d_18 (MaxPooling (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","conv2d_58 (Conv2D)           (None, 28, 28, 521)       1200905   \n","_________________________________________________________________\n","conv2d_59 (Conv2D)           (None, 28, 28, 521)       2443490   \n","_________________________________________________________________\n","conv2d_60 (Conv2D)           (None, 28, 28, 521)       2443490   \n","_________________________________________________________________\n","conv2d_61 (Conv2D)           (None, 28, 28, 521)       2443490   \n","_________________________________________________________________\n","max_pooling2d_19 (MaxPooling (None, 14, 14, 521)       0         \n","_________________________________________________________________\n","conv2d_62 (Conv2D)           (None, 14, 14, 521)       2443490   \n","_________________________________________________________________\n","conv2d_63 (Conv2D)           (None, 14, 14, 521)       2443490   \n","_________________________________________________________________\n","conv2d_64 (Conv2D)           (None, 14, 14, 521)       2443490   \n","_________________________________________________________________\n","conv2d_65 (Conv2D)           (None, 14, 14, 521)       2443490   \n","_________________________________________________________________\n","max_pooling2d_20 (MaxPooling (None, 7, 7, 521)         0         \n","_________________________________________________________________\n","flatten_4 (Flatten)          (None, 25529)             0         \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 4096)              104570880 \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 4096)              16781312  \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 1000)              4097000   \n","=================================================================\n","Total params: 146,080,095\n","Trainable params: 146,080,095\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G7iqHL09XO_c"},"source":["##Using VGG16 model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"id":"Z0TfYyurYCxL","executionInfo":{"status":"ok","timestamp":1621978559068,"user_tz":-420,"elapsed":29562,"user":{"displayName":"Dũng Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gii5U-zs-8ujhQaCpvDo03iequwy5RSKEnhRnwQGQ=s64","userId":"11702303910424597473"}},"outputId":"242b3748-e3f8-4c0c-d4c2-e47e46da0066"},"source":["from google.colab import drive\n","import os\n","\n","drive.mount(\"/content/drive\")\n","path = '/content/drive/MyDrive/Colab Notebooks/Data'\n","os.chdir(path)\n","os.listdir()\n","\n","import pandas as pd\n","import glob2\n","import matplotlib.pyplot as plt\n","\n","dogs = glob2.glob('Train_Data/dog/*.jpg')\n","dog_labels = ['dog']*len(dogs)\n","cats = glob2.glob('Train_Data/cat/*.jpg')\n","cat_labels = ['cat']*len(cats)\n","\n","labels = dog_labels + cat_labels\n","image_links = dogs + cats\n","\n","data = pd.DataFrame({'labels': labels, 'image_links':image_links})\n","\n","data.groupby(labels).image_links.count().plot.bar()\n","plt.title('Number of images in each class')\n","plt.show()\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"a5dWXEGawtFT"},"source":["Test thử Model VGG16"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bXCgfgAXWNc","executionInfo":{"status":"ok","timestamp":1621978910280,"user_tz":-420,"elapsed":3397,"user":{"displayName":"Dũng Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gii5U-zs-8ujhQaCpvDo03iequwy5RSKEnhRnwQGQ=s64","userId":"11702303910424597473"}},"outputId":"a844697d-4a8c-49b4-c08f-efb8b0c3e5da"},"source":["from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from keras.applications.vgg16 import decode_predictions\n","from keras.applications.vgg16 import VGG16\n","# load the model\n","model = VGG16()\n","# load an image from file\n","image = load_img(dogs[2], target_size=(224, 224))\n","# convert the image pixels to a numpy array\n","image = img_to_array(image)\n","# reshape data for the model\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","# prepare the image for the VGG model\n","image = preprocess_input(image)\n","# predict the probability across all output classes\n","yhat = model.predict(image)\n","# convert the probabilities to class labels\n","label = decode_predictions(yhat)\n","# retrieve the most likely result, e.g. highest probability\n","label = label[0][0]\n","# print the classification\n","print('%s (%.2f%%)' % (label[1], label[2]*100))\n","model.summary()\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Chesapeake_Bay_retriever (90.68%)\n","Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 4096)              102764544 \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 4096)              16781312  \n","_________________________________________________________________\n","predictions (Dense)          (None, 1000)              4097000   \n","=================================================================\n","Total params: 138,357,544\n","Trainable params: 138,357,544\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8usULuG7en3a"},"source":["Áp dụng Fineturn learning vào VGG16"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gc7sYLlzesmf","executionInfo":{"status":"ok","timestamp":1621985049276,"user_tz":-420,"elapsed":911,"user":{"displayName":"Dũng Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gii5U-zs-8ujhQaCpvDo03iequwy5RSKEnhRnwQGQ=s64","userId":"11702303910424597473"}},"outputId":"4261ced6-51d5-49c3-aa57-c17ae5f5bc14"},"source":["from keras.preprocessing import image\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.optimizers import SGD\n","\n","base_model = VGG16(weights='imagenet',input_shape=(224,224,3), include_top=False)\n","# # loop over all layers in the base model and freeze them so they will\n","# # *not* be updated during the first training process\n","for layer in base_model.layers:\n","\tlayer.trainable = False\n","\n","x = base_model.output\n","x = Flatten()(x)\n","x = Dense(512, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","x = Dense(512, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","\n","predictions = Dense(2, activation='softmax')(x)\n","\n","# this is the model we will train\n","FTmodel = Model(inputs=base_model.input, outputs=predictions)\n","\n","FTmodel.compile(loss=\"categorical_crossentropy\", optimizer='adam',\n","\tmetrics=[\"accuracy\"])\n","\n","FTmodel.summary()\n"],"execution_count":97,"outputs":[{"output_type":"stream","text":["Model: \"model_37\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_53 (InputLayer)        [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten_40 (Flatten)         (None, 25088)             0         \n","_________________________________________________________________\n","dense_128 (Dense)            (None, 512)               12845568  \n","_________________________________________________________________\n","dropout_34 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_129 (Dense)            (None, 512)               262656    \n","_________________________________________________________________\n","dropout_35 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","dense_130 (Dense)            (None, 2)                 1026      \n","=================================================================\n","Total params: 27,823,938\n","Trainable params: 13,109,250\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zbQ76CzigA2Q","executionInfo":{"status":"ok","timestamp":1621984123549,"user_tz":-420,"elapsed":586380,"user":{"displayName":"Dũng Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gii5U-zs-8ujhQaCpvDo03iequwy5RSKEnhRnwQGQ=s64","userId":"11702303910424597473"}},"outputId":"5ba4d41d-311e-417b-9dd7-19c9c2582487"},"source":["from sklearn.preprocessing import LabelBinarizer\n","Y = LabelBinarizer().fit_transform(data.labels)\n","\n","X=[]\n","for link in data.image_links:\n","  # load an image from file\n","  image = load_img(link, target_size=(224, 224))\n","  # convert the image pixels to a numpy array\n","  image = img_to_array(image)\n","  # reshape data for the model\n","  image = image.reshape((image.shape[0], image.shape[1], image.shape[2]))\n","  # prepare the image for the VGG model\n","  image = preprocess_input(image)\n","  X.append(image)\n","\n","X=np.asarray(X)\n","print(X.shape)\n"],"execution_count":81,"outputs":[{"output_type":"stream","text":["(1399, 224, 224, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pMICE8hqsLeP","executionInfo":{"status":"ok","timestamp":1621986583134,"user_tz":-420,"elapsed":1529769,"user":{"displayName":"Dũng Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gii5U-zs-8ujhQaCpvDo03iequwy5RSKEnhRnwQGQ=s64","userId":"11702303910424597473"}},"outputId":"3054766c-b796-4a58-d55f-c8a781d90aff"},"source":["from keras.utils.np_utils import to_categorical\n","train_labels = to_categorical(Y)\n","\n","FTmodel.fit(X,train_labels,epochs=3)"],"execution_count":98,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","44/44 [==============================] - 511s 12s/step - loss: 15.0727 - accuracy: 0.8033\n","Epoch 2/3\n","44/44 [==============================] - 509s 12s/step - loss: 2.7852 - accuracy: 0.9696\n","Epoch 3/3\n","44/44 [==============================] - 510s 12s/step - loss: 1.9843 - accuracy: 0.9847\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fb83d2e7c50>"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYIaBfko8w3C","executionInfo":{"status":"ok","timestamp":1621988473680,"user_tz":-420,"elapsed":1582392,"user":{"displayName":"Dũng Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gii5U-zs-8ujhQaCpvDo03iequwy5RSKEnhRnwQGQ=s64","userId":"11702303910424597473"}},"outputId":"6bd3b6c7-a6d7-429e-95f2-cb820a784f6a"},"source":["for layer in FTmodel.layers:\n","\tlayer.trainable = True\n","\n","FTmodel.fit(X,train_labels,epochs=3)\n"],"execution_count":99,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","44/44 [==============================] - 508s 12s/step - loss: 2.1406 - accuracy: 0.9778\n","Epoch 2/3\n","44/44 [==============================] - 508s 12s/step - loss: 0.9254 - accuracy: 0.9871\n","Epoch 3/3\n","44/44 [==============================] - 508s 12s/step - loss: 0.6939 - accuracy: 0.9914\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fb83b0d5b90>"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"id":"aA2djO9489Py"},"source":[""],"execution_count":null,"outputs":[]}]}